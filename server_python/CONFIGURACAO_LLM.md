# üîß Configura√ß√£o do LLM

O sistema precisa de um LLM (Large Language Model) para processar os boletins. Voc√™ tem duas op√ß√µes:

## Op√ß√£o 1: OpenAI (Recomendado - Mais r√°pido)

### Passos:

1. **Obtenha uma chave da API OpenAI:**
   - Acesse: https://platform.openai.com/account/api-keys
   - Fa√ßa login ou crie uma conta
   - Clique em "Create new secret key"
   - Copie a chave (ela come√ßa com `sk-`)

2. **Configure no arquivo `.env`:**
   ```bash
   cd server_python
   nano .env  # ou use seu editor preferido
   ```

3. **Edite o arquivo:**
   ```env
   LLM_PROVIDER=openai
   OPENAI_API_KEY=sk-sua-chave-real-aqui
   ```

4. **Reinicie o servidor:**
   ```bash
   # Pare o servidor (Ctrl+C)
   npm run dev
   ```

### Custos:
- OpenAI cobra por uso (aproximadamente $0.15 por 1M tokens)
- Para processar boletins, o custo √© muito baixo (centavos por boletim)

---

## Op√ß√£o 2: Ollama (Gratuito - Local)

### Passos:

1. **Instale o Ollama:**
   ```bash
   # macOS
   brew install ollama
   
   # Ou baixe em: https://ollama.ai
   ```

2. **Inicie o Ollama:**
   ```bash
   ollama serve
   ```

3. **Baixe um modelo (em outro terminal):**
   ```bash
   ollama pull llama3.2
   # ou
   ollama pull llama3.1:8b  # Vers√£o menor, mais r√°pida
   ```

4. **Configure no arquivo `.env`:**
   ```bash
   cd server_python
   nano .env
   ```

5. **Edite o arquivo:**
   ```env
   LLM_PROVIDER=ollama
   # OPENAI_API_KEY n√£o √© necess√°ria
   ```

6. **Reinicie o servidor:**
   ```bash
   # Pare o servidor (Ctrl+C)
   npm run dev
   ```

### Vantagens:
- ‚úÖ Gratuito
- ‚úÖ Funciona offline
- ‚úÖ Dados n√£o saem do seu computador

### Desvantagens:
- ‚ö†Ô∏è Mais lento que OpenAI
- ‚ö†Ô∏è Requer mais recursos do computador
- ‚ö†Ô∏è Precis√£o pode ser um pouco menor

---

## Verifica√ß√£o

Ap√≥s configurar, verifique se est√° funcionando:

```bash
curl http://localhost:5001/api/health
```

Voc√™ deve ver:
```json
{
  "status": "OK",
  "message": "Servidor rodando",
  "llm_provider": "openai" ou "ollama",
  "ocr_engine": "paddleocr"
}
```

---

## Troubleshooting

### Erro: "Incorrect API key provided"
- Verifique se a chave no `.env` est√° correta
- Certifique-se de que n√£o h√° espa√ßos extras
- A chave deve come√ßar com `sk-`

### Erro: "Ollama connection refused"
- Certifique-se de que o Ollama est√° rodando: `ollama serve`
- Verifique se o modelo foi baixado: `ollama list`

### Erro: "Model not found"
- Baixe o modelo: `ollama pull llama3.2`

